{'dataset_params': {'im_path': 'data/train/images'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02}, 'model_params': {'im_channels': 3, 'im_size': 32, 'down_channels': [32, 64, 128, 256], 'mid_channels': [256, 256, 256], 'down_sample': [True, True, False], 'time_emb_dim': 128, 'num_down_layers': 3, 'num_mid_layers': 3, 'num_up_layers': 3, 'num_heads': 4}, 'train_params': {'task_name': 'cifar10', 'batch_size': 64, 'num_epochs': 50, 'num_samples': 1000, 'num_samples_progress': 10, 'num_grid_rows': 10, 'lr': 0.0001, 'ckpt_name': 'ddpm_ckpt.pth'}}
Warning: Could not load checkpoint from cifar10/ddpm_ckpt.pth: Weights only load failed. This file can still be loaded, to do so you have two options, [1mdo those steps only if you trust the source of the checkpoint[0m. 
	(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.
	(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.
	WeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy._core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.

Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.
Starting training from scratch
Epoch 1/50, Loss: 0.11871311023278767
Epoch 2/50, Loss: 0.05105052199786353
Epoch 3/50, Loss: 0.04348640242243743
Epoch 4/50, Loss: 0.040550442698323516
Epoch 5/50, Loss: 0.03910320017086651
Epoch 6/50, Loss: 0.03775639895854704
Epoch 7/50, Loss: 0.03680093305738038
Epoch 8/50, Loss: 0.03576310924218629
Epoch 9/50, Loss: 0.03528722930971123
Epoch 10/50, Loss: 0.03525414309747841
Epoch 11/50, Loss: 0.03469538528834234
Epoch 12/50, Loss: 0.034228111317147834
Epoch 13/50, Loss: 0.03457853410159573
Epoch 14/50, Loss: 0.033872933498562297
Epoch 15/50, Loss: 0.03343146543025666
Epoch 16/50, Loss: 0.03343448099916053
Epoch 17/50, Loss: 0.03364248923204668
Epoch 18/50, Loss: 0.03306970171049199
Epoch 19/50, Loss: 0.03298022500013032
Epoch 20/50, Loss: 0.03297963851581678
Epoch 21/50, Loss: 0.03306083403684942
Epoch 22/50, Loss: 0.03244609667507507
Epoch 23/50, Loss: 0.03238998071940811
Epoch 24/50, Loss: 0.03203096230993109
Epoch 25/50, Loss: 0.03230074250741916
Epoch 26/50, Loss: 0.032147866770234484
Epoch 27/50, Loss: 0.03221250972484269
Epoch 28/50, Loss: 0.03216344535784305
Epoch 29/50, Loss: 0.03208341676613216
Epoch 30/50, Loss: 0.031598757541574096
Epoch 31/50, Loss: 0.03170514640653187
Epoch 32/50, Loss: 0.031085296413715918
Epoch 33/50, Loss: 0.03183876888950348
Epoch 34/50, Loss: 0.03167288797095304
Epoch 35/50, Loss: 0.03163750714305645
Epoch 36/50, Loss: 0.03158374581738468
Epoch 37/50, Loss: 0.03208197285409283
Epoch 38/50, Loss: 0.031458904545115844
Epoch 39/50, Loss: 0.031601548337799205
Epoch 40/50, Loss: 0.031618131190071556
Epoch 41/50, Loss: 0.03103503396691721
Epoch 42/50, Loss: 0.03138543912769317
Epoch 43/50, Loss: 0.031436939395087606
Epoch 44/50, Loss: 0.03127319144699579
Epoch 45/50, Loss: 0.031920108286773455
Epoch 46/50, Loss: 0.031019634002691036
Epoch 47/50, Loss: 0.031265623481644085
Epoch 48/50, Loss: 0.031236029076425698
Epoch 49/50, Loss: 0.03109767989438894
Epoch 50/50, Loss: 0.031282672369161914
